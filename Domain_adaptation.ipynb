{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%sh\n",
    "\n",
    "wget -q -nc https://raw.githubusercontent.com/amitgroup/amitgroup/master/amitgroup/io/mnist.py -O mnisty.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%sh\n",
    "\n",
    "mkdir -p mnist && {\n",
    "    cd mnist;\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz &&\n",
    "    gunzip *.gz\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import *\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### http://g.sweyla.com/blog/2012/mnist-numpy/\n",
    "import mnisty as mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = mnist.load_mnist(dataset='training', path='mnist/')\n",
    "X = X.reshape(-1, 1, 28, 28).astype('float32')\n",
    "\n",
    "X_test, y_test = mnist.load_mnist(dataset='testing', path='mnist/')\n",
    "X_test = X_test.reshape(-1, 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_inv, y_inv = mnist.load_mnist(dataset='training', path='mnist/')\n",
    "X_inv = 1.-X_inv.reshape(-1, 1, 28, 28).astype('float32')\n",
    "\n",
    "X_inv_test, y_inv_test = mnist.load_mnist(dataset='testing', path='mnist/')\n",
    "X_inv_test = 1.-X_inv_test.reshape(-1, 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y, n_classes=10):\n",
    "    onehot = np.zeros(shape=(y.shape[0], n_classes), dtype='float32')\n",
    "\n",
    "    onehot[np.arange(y.shape[0]), y] = 1\n",
    "    return onehot\n",
    "\n",
    "y = one_hot(y).astype('int32')\n",
    "y_test = one_hot(y_test).astype('int32')\n",
    "\n",
    "y_inv = one_hot(y_inv).astype('int32')\n",
    "y_inv_test = one_hot(y_inv_test).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:50000]\n",
    "X_val = X[50000:]\n",
    "\n",
    "y_train = y[:50000]\n",
    "y_val = y[50000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_inv_train = X_inv[:50000]\n",
    "X_inv_val = X_inv[50000:]\n",
    "\n",
    "y_inv_train = y_inv[:50000]\n",
    "y_inv_val = y_inv[50000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset domain - \n",
    "\n",
    "X_dom_train = np.concatenate((X_train, X_inv_train))\n",
    "X_dom_val = np.concatenate((X_val, X_inv_val))\n",
    "\n",
    "y_dom_train = np.concatenate(([1]*X_train.shape[0],[0]*X_inv_train.shape[0] )).astype('int32')\n",
    "y_dom_val = np.concatenate(([1]*X_val.shape[0],[0]*X_inv_val.shape[0] )).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0].max(), X_inv[0][0].max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.imshow(X[0][0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.imshow(X_inv[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Usual vgg like arch for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_shape = (None, 1, 28, 28)\n",
    "\n",
    "input_img = T.tensor4('x_image')\n",
    "target_y = T.matrix(\"target\", dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pool_1', (None, 8, 13, 13))\n",
      "('pool_2', (None, 16, 5, 5))\n",
      "('pool_3', (None, 32, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "nnet = OrderedDict()\n",
    "nnet['input'] = lasagne.layers.InputLayer(img_shape, input_img)\n",
    "nnet['conv_11'] = lasagne.layers.Conv2DLayer(nnet[nnet.keys()[-1]], 8, 2)\n",
    "nnet['conv_12'] = lasagne.layers.Conv2DLayer(nnet[nnet.keys()[-1]], 8, 2)\n",
    "nnet['pool_1'] = lasagne.layers.Pool2DLayer(nnet[nnet.keys()[-1]], 2)\n",
    "print (nnet.keys()[-1], nnet[nnet.keys()[-1]].output_shape)\n",
    "\n",
    "nnet['conv_21'] = lasagne.layers.Conv2DLayer(nnet[nnet.keys()[-1]], 16, 2)\n",
    "nnet['conv_22'] = lasagne.layers.Conv2DLayer(nnet[nnet.keys()[-1]], 16, 2)\n",
    "nnet['pool_2'] = lasagne.layers.Pool2DLayer(nnet[nnet.keys()[-1]], 2)\n",
    "print (nnet.keys()[-1], nnet[nnet.keys()[-1]].output_shape)\n",
    "\n",
    "nnet['conv_31'] = lasagne.layers.Conv2DLayer(nnet[nnet.keys()[-1]], 32, 2)\n",
    "nnet['conv_32'] = lasagne.layers.Conv2DLayer(nnet[nnet.keys()[-1]], 32, 2)\n",
    "nnet['pool_3'] = lasagne.layers.Pool2DLayer(nnet[nnet.keys()[-1]], 2)\n",
    "print (nnet.keys()[-1], nnet[nnet.keys()[-1]].output_shape)\n",
    "\n",
    "nnet['dense_1'] = lasagne.layers.DenseLayer(nnet[nnet.keys()[-1]], 100)\n",
    "nnet['dense_2'] = lasagne.layers.DenseLayer(nnet[nnet.keys()[-1]], y.shape[1], \n",
    "                                            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "nnet_output = nnet[nnet.keys()[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b, W, b, W, b, W, b, W, b, W, b]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/git_downloads/docker-jupyter-keras-tools/src/lasagne/lasagne/layers/pool.py:267: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode=self.mode,\n",
      "/home/amir/git_downloads/docker-jupyter-keras-tools/src/lasagne/lasagne/layers/pool.py:267: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode=self.mode,\n",
      "/home/amir/git_downloads/docker-jupyter-keras-tools/src/lasagne/lasagne/layers/pool.py:267: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode=self.mode,\n"
     ]
    }
   ],
   "source": [
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(nnet_output)\n",
    "\n",
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(nnet_output,trainable=True)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted, target_y).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prediction accuracy (WITH dropout)\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted, target_y).mean()\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adamax(loss, all_weights, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_img, target_y],[loss, accuracy], updates = updates_sgd)\n",
    "#function that computes loss and updates weights\n",
    "val_fun = theano.function([input_img, target_y], [loss, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_fun = theano.function([input_img], y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open('some_file_for_output.txt', 'w') as f:\n",
    "    f.write(\"\\n  go! \\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "import time\n",
    " \n",
    "num_epochs = 50 # amount of passes through the data\n",
    "            \n",
    "batch_size = 100 # number of samples processed at each function call\n",
    "print 'starting trainig...'\n",
    "\n",
    "train_loss_ = []\n",
    "val_loss_ = []\n",
    "val_acc_ = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    batch_size += 10\n",
    "    \n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch = train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    val_err = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        _val_err, _val_acc = val_fun(inputs, targets)\n",
    "        val_acc += _val_acc\n",
    "        val_err += _val_err\n",
    "        val_batches += 1\n",
    "         \n",
    "    train_loss_.append(train_err / train_batches)\n",
    "    val_loss_.append(val_err / val_batches)\n",
    "    val_acc_.append(val_acc / val_batches * 100)\n",
    "\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(train_acc / train_batches * 100))\n",
    "    print(\"  validation loss (in-iteration):\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100))\n",
    "\n",
    "    \n",
    "    with open('some_file_for_output.txt', 'a') as f:\n",
    "        f.write(\"\\n  Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        f.write(\"\\n  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        f.write(\"\\n  train accuracy:\\t\\t{:.2f} %\".format(train_acc / train_batches * 100))\n",
    "        f.write(\"\\n  validation loss (in-iteration):\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        f.write(\"\\n  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  go! \n",
      "\n",
      "\n",
      "\n",
      "  Epoch 1 of 50 took 1818.059s\n",
      "\n",
      "  training loss (in-iteration):\t\t0.494242\n",
      "\n",
      "  train accuracy:\t\t84.30 %\n",
      "\n",
      "  validation loss (in-iteration):\t\t0.188630\n",
      "\n",
      "  validation accuracy:\t\t94.43 %\n",
      "\n",
      "  Epoch 2 of 50 took 1799.743s\n",
      "\n",
      "  training loss (in-iteration):\t\t0.171911\n",
      "\n",
      "  train accuracy:\t\t94.68 %\n",
      "\n",
      "  validation loss (in-iteration):\t\t0.124711\n",
      "\n",
      "  validation accuracy:\t\t96.28 %\n",
      "\n",
      "  Epoch 3 of 50 took 1798.215s\n",
      "\n",
      "  training loss (in-iteration):\t\t0.129977\n",
      "\n",
      "  train accuracy:\t\t96.04 %\n",
      "\n",
      "  validation loss (in-iteration):\t\t0.111339\n",
      "\n",
      "  validation accuracy:\t\t96.79 %\n",
      "\n",
      "  Epoch 4 of 50 took 1799.643s\n",
      "\n",
      "  training loss (in-iteration):\t\t0.106991\n",
      "\n",
      "  train accuracy:\t\t96.79 %\n",
      "\n",
      "  validation loss (in-iteration):\t\t0.099906\n",
      "\n",
      "  validation accuracy:\t\t97.03 %\n",
      "\n",
      "  Epoch 5 of 50 took 1797.478s\n",
      "\n",
      "  training loss (in-iteration):\t\t0.092860\n",
      "\n",
      "  train accuracy:\t\t97.13 %\n",
      "\n",
      "  validation loss (in-iteration):\t\t0.096204\n",
      "\n",
      "  validation accuracy:\t\t97.05 %\n"
     ]
    }
   ],
   "source": [
    "with open('some_file_for_output.txt', 'r') as f:\n",
    "    for i in f.readlines():\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7e061a2fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAH4CAYAAAAVXz/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w+/fniWjQEHHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e062f3f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps = np.arange(len(train_loss_))\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim((0, 2))\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(eps, train_loss_, label='train')\n",
    "plt.plot(eps, val_loss_, label='val')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(eps, val_acc_, label='val')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "# plt.savefig('./some_picture.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    ">> на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some_preds = pred_fun(X_test[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97099999999999997"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(y_test[:1000], axis=1) == np.argmax(some_preds, axis=1)) * 1.0 / len(np.argmax(some_preds, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> на тесте другого датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_preds = pred_fun(X_inv[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16900000000000001"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(y_inv[:1000], axis=1) == np.argmax(some_preds, axis=1)) * 1.0 / len(np.argmax(some_preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving\n",
    "np.savez('model_cnn_mnist1.npz', *lasagne.layers.get_all_param_values(nnet['dense_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "итого 204\r\n",
      "drwxrwxr-x 4 amir amir   4096 дек  5 23:35 .\r\n",
      "drwxrwxr-x 9 amir amir   4096 дек  5 19:57 ..\r\n",
      "-rw-rw-r-- 1 amir amir 118688 дек  5 23:34 Domain_adaptation.ipynb\r\n",
      "drwxr-xr-x 2 amir amir   4096 дек  5 20:06 .ipynb_checkpoints\r\n",
      "drwxrwxr-x 2 amir amir   4096 дек  5 19:59 mnist\r\n",
      "-rw-rw-r-- 1 amir amir   4874 дек  5 19:58 mnisty.py\r\n",
      "-rw-rw-r-- 1 amir amir   4925 дек  5 19:58 mnisty.pyc\r\n",
      "-rw-rw-r-- 1 amir amir  52378 дек  5 23:35 model_cnn_mnist1.npz\r\n",
      "-rw-rw-r-- 1 amir amir    888 дек  5 23:15 some_file_for_output.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#And load them again later on like this:\n",
    "with np.load('model_1.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(nn, param_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "_______\n",
    "\n",
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Adaptation by Ganin, Lempitsky\n",
    "\n",
    "https://arxiv.org/pdf/1409.7495.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/akoSm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_shape = (None, 1, 28, 28)\n",
    "\n",
    "input_img = T.tensor4('X_image')\n",
    "# target_y = T.vector(\"target_class\", dtype='int32')\n",
    "target_y = T.matrix(\"target\", dtype='int32')\n",
    "target_domain = T.vector(\"target_domain\", dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pool_1', (None, 8, 13, 13))\n",
      "('pool_2', (None, 32, 5, 5))\n"
     ]
    }
   ],
   "source": [
    "da_nnet = OrderedDict()\n",
    "da_nnet['input'] = lasagne.layers.InputLayer(img_shape, input_img)\n",
    "da_nnet['conv_11'] = lasagne.layers.Conv2DLayer(da_nnet[da_nnet.keys()[-1]], 4, 2, name='conv_11')\n",
    "da_nnet['conv_12'] = lasagne.layers.Conv2DLayer(da_nnet[da_nnet.keys()[-1]], 8, 2, name='conv_12')\n",
    "da_nnet['pool_1'] = lasagne.layers.Pool2DLayer(da_nnet[da_nnet.keys()[-1]], 2, name='pool_1')\n",
    "print (da_nnet.keys()[-1], da_nnet[da_nnet.keys()[-1]].output_shape)\n",
    "\n",
    "da_nnet['conv_21'] = lasagne.layers.Conv2DLayer(da_nnet[da_nnet.keys()[-1]], 16, 2, name='conv_21')\n",
    "da_nnet['conv_22'] = lasagne.layers.Conv2DLayer(da_nnet[da_nnet.keys()[-1]], 32, 2, name='conv_22')\n",
    "da_nnet['pool_2'] = lasagne.layers.Pool2DLayer(da_nnet[da_nnet.keys()[-1]], 2, name='pool_2')\n",
    "print (da_nnet.keys()[-1], da_nnet[da_nnet.keys()[-1]].output_shape)\n",
    "\n",
    "# da_nnet['conv_31'] = lasagne.layers.Conv2DLayer(da_nnet[da_nnet.keys()[-1]], 32, 2, name='conv_31')\n",
    "# da_nnet['conv_32'] = lasagne.layers.Conv2DLayer(da_nnet[da_nnet.keys()[-1]], 32, 2, name='conv_32')\n",
    "# da_nnet['pool_3'] = lasagne.layers.Pool2DLayer(da_nnet[da_nnet.keys()[-1]], 2, name='pool_3')\n",
    "# print (da_nnet.keys()[-1], da_nnet[da_nnet.keys()[-1]].output_shape)\n",
    "\n",
    "da_nnet['feature'] = lasagne.layers.DenseLayer(da_nnet[da_nnet.keys()[-1]], 32, name='features')\n",
    "\n",
    "######\n",
    "\n",
    "da_nnet['classifier_dense_1'] = lasagne.layers.DenseLayer(da_nnet['feature'], 64, name='clf_dense_1')\n",
    "da_nnet['classifier_dense_2'] = lasagne.layers.DenseLayer(da_nnet['classifier_dense_1'], 48, name='clf_dense_2')\n",
    "\n",
    "da_nnet['classifier_ans'] = lasagne.layers.DenseLayer(da_nnet['classifier_dense_2'], y.shape[1], \n",
    "                                    nonlinearity=lasagne.nonlinearities.softmax, name='clf_ans')\n",
    "\n",
    "######\n",
    "\n",
    "da_nnet['domain_dense_1'] = lasagne.layers.DenseLayer(da_nnet['feature'], 64, name='dom_dense_1')\n",
    "da_nnet['domain_dense_2'] = lasagne.layers.DenseLayer(da_nnet['domain_dense_1'], 32, name='dom_dense_2')\n",
    "\n",
    "da_nnet['domain_ans'] = lasagne.layers.DenseLayer(da_nnet['domain_dense_2'], 1, \n",
    "                                       nonlinearity=lasagne.nonlinearities.sigmoid, \n",
    "                                                                 name='domain_ans')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_predicted = lasagne.layers.get_output(da_nnet['classifier_ans'])\n",
    "dom_predicted = lasagne.layers.get_output(da_nnet['domain_ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_targ = theano.function([input_img], clf_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_classifier = lasagne.objectives.categorical_crossentropy(clf_predicted, target_y).mean() \n",
    "loss_domain = lasagne.objectives.binary_crossentropy(dom_predicted, target_domain).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_weights = lasagne.layers.get_all_params(da_nnet['classifier_ans'], trainable=True)\n",
    "dom_weights = lasagne.layers.get_all_params(da_nnet['domain_ans'], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_common_params = set(clf_weights) & set(dom_weights)\n",
    "###\n",
    "# Те веса которые с минусом должны быть в бекпропе классификатора\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambd = theano.shared(1., name='lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates_dom_sgd_with_minus = lasagne.updates.adamax( - lambd * loss_domain, list(net_common_params), \n",
    "                                                    learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates_dom_sgd = updates_dom_sgd_with_minus.update(lasagne.updates.adamax(loss_domain, \n",
    "                                            list(set(dom_weights) - net_common_params), \n",
    "                                            learning_rate=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates_clf_sgd = lasagne.updates.adamax(loss_classifier, clf_weights, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_loss = lasagne.objectives.categorical_accuracy(clf_predicted, target_y).mean() \n",
    "accuracy_bin_loss = lasagne.objectives.binary_accuracy(dom_predicted, target_domain).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_clf_fun = theano.function([input_img, target_y], loss_classifier, updates = updates_clf_sgd)\n",
    "val_clf_fun = theano.function([input_img, target_y], [loss_classifier, accuracy_loss])\n",
    "\n",
    "train_dom_fun = theano.function([input_img, target_domain], loss_domain, updates = updates_dom_sgd)\n",
    "val_dom_fun = theano.function([input_img, target_domain], [loss_domain, accuracy_bin_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(2.3030494646592574), array(0.6583121716976166))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Проверим что все запускается\n",
    "\n",
    "train_clf_fun(X_train[:110], y_train[:110]), train_dom_fun(X_dom_train[:2], y_dom_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('some_file_for_output1.txt', 'w') as f:\n",
    "    f.write(\"\\n  start at {}! \\n\".format(str(datetime.datetime.now())[:19]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting trainig...\n",
      "\n",
      "  training clf loss (in-iteration):\t\t0.389165\n",
      "\n",
      "\t\t  757.130s\n",
      "\n",
      "  validation on base domain dataset:\n",
      "\n",
      "\t\t  validation clf loss (in-iteration):\t\t0.116115\n",
      "\n",
      "\t\t  val accuracy (in-iteration):\t\t0.966768\n",
      "\n",
      "\t\t  54.552s\n",
      "\n",
      "  validation on inverse dataset:\n",
      "\n",
      "\t\t  validation clf loss (in-iteration):\t\t8.460004\n",
      "\n",
      "\t\t  val accuracy (in-iteration):\t\t0.051626\n",
      "\n",
      "\t\t  54.954s\n",
      "\n",
      "  training domain loss (in-iteration):\t\t1.333369\n",
      "\n",
      "\t\t  556.107s\n",
      "\n",
      "  validation domain loss (in-iteration):\t\t1.444543\n",
      "\n",
      "\t\t  validation acc (in-iteration):\t\t0.500126\n",
      "\n",
      "\t\t  112.874s\n",
      "\n",
      "  validation on base domain dataset:\n",
      "\n",
      "\t\t  validation clf loss (in-iteration):\t\t0.115595\n",
      "\n",
      "\t\t  val accuracy (in-iteration):\t\t0.966972\n",
      "\n",
      "\t\t  55.747s\n",
      "\n",
      "  validation on inverse dataset:\n",
      "\n",
      "\t\t  validation clf loss (in-iteration):\t\t8.459207\n",
      "\n",
      "\t\t  val accuracy (in-iteration):\t\t0.052439\n",
      "\n",
      "\t\t  54.172s\n",
      "\n",
      "  training clf loss (in-iteration):\t\t0.104407\n",
      "\n",
      "\t\t  776.175s\n",
      "\n",
      "  validation on base domain dataset:\n",
      "\n",
      "\t\t  validation clf loss (in-iteration):\t\t0.081459\n",
      "\n",
      "\t\t  val accuracy (in-iteration):\t\t0.976292\n",
      "\n",
      "\t\t  55.434s\n",
      "\n",
      "  validation on inverse dataset:\n",
      "\n",
      "\t\t  validation clf loss (in-iteration):\t\t7.870161\n",
      "\n",
      "\t\t  val accuracy (in-iteration):\t\t0.062107\n",
      "\n",
      "\t\t  54.891s\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "num_epochs = 50 # amount of passes through the data\n",
    "            \n",
    "batch_size = 200 # number of samples processed at each function call\n",
    "print 'starting trainig...'\n",
    "\n",
    "#train_loss_ = []\n",
    "val_acc_base_ = []\n",
    "val_acc_inv_ = []\n",
    "\n",
    "for epoch in tqdm_notebook(range(num_epochs), leave=False, desc='epoches'):\n",
    "    \n",
    "    \n",
    "    batch_size += 5\n",
    "\n",
    "    #################\n",
    "    #   Classifier  #\n",
    "    #################\n",
    "    \n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm_notebook(iterate_minibatches(X_train, y_train, batch_size), \n",
    "                                total=int(y_train.shape[0] / batch_size), leave=False, \n",
    "                                                             desc = 'classifier_training'):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch = train_clf_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_batches += 1\n",
    "        \n",
    "    print (\"\\n  training clf loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print (\"\\n\\t\\t  {:.3f}s\".format(time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    ################\n",
    "    #  validation  #\n",
    "    ################\n",
    "    #\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm_notebook(iterate_minibatches(X_val, y_val, batch_size), \n",
    "                                total=int(y_val.shape[0]/batch_size), leave=False, \n",
    "                                       desc = 'classifier_validation_on_base_dataset'):\n",
    "        inputs, targets = batch\n",
    "        val_err_batch, val_acc_batch = val_clf_fun(inputs, targets)\n",
    "        val_err += val_err_batch\n",
    "        val_acc += val_acc_batch\n",
    "        val_batches += 1\n",
    "    print (\"\\n  validation on base domain dataset:\")\n",
    "    print (\"\\n\\t\\t  validation clf loss (in-iteration):\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print (\"\\n\\t\\t  val accuracy (in-iteration):\\t\\t{:.6f}\".format(val_acc / val_batches))\n",
    "    print (\"\\n\\t\\t  {:.3f}s\".format(time.time() - start_time))\n",
    "    \n",
    "    val_acc_base_.append(val_acc / val_batches)\n",
    "\n",
    "    \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm_notebook(iterate_minibatches(X_inv_val, y_inv_val, batch_size), \n",
    "                                    total=int(y_inv_val.shape[0]/batch_size), leave=False, \n",
    "                                            desc = 'classifier_validation_on_inverse_dataset'):\n",
    "        inputs, targets = batch\n",
    "        val_err_batch, val_acc_batch = val_clf_fun(inputs, targets)\n",
    "        val_err += val_err_batch\n",
    "        val_acc += val_acc_batch\n",
    "        val_batches += 1\n",
    "    print (\"\\n  validation on inverse dataset:\")    \n",
    "    print (\"\\n\\t\\t  validation clf loss (in-iteration):\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print (\"\\n\\t\\t  val accuracy (in-iteration):\\t\\t{:.6f}\".format(val_acc / val_batches))\n",
    "    print (\"\\n\\t\\t  {:.3f}s\".format(time.time() - start_time))\n",
    "    \n",
    "    val_acc_inv_.append(val_acc / val_batches)\n",
    "    \n",
    "    #################\n",
    "    #  DOMAIN LABEL #\n",
    "    #################\n",
    "    \n",
    "    _train_err = 0\n",
    "    _train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm_notebook(iterate_minibatches(X_dom_train, y_dom_train, batch_size),\n",
    "                                    total = int( y_dom_train.shape[0]/batch_size), leave=False, \n",
    "                                                    desc = 'domain_label_classification_training'):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch = train_dom_fun(inputs, targets)\n",
    "        _train_err += train_err_batch\n",
    "        _train_batches += 1\n",
    "        \n",
    "    print (\"\\n  training domain loss (in-iteration):\\t\\t{:.6f}\".format(_train_err / _train_batches))    \n",
    "    print (\"\\n\\t\\t  {:.3f}s\".format(time.time() - start_time))\n",
    "    \n",
    "    # domain validation \n",
    "    _val_err = 0\n",
    "    _val_acc = 0\n",
    "    _val_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm_notebook(iterate_minibatches(X_dom_val, y_dom_val, batch_size),\n",
    "                                  total = int(y_dom_val.shape[0]/batch_size), leave=False,\n",
    "                                                             desc = 'domain_label_validation'):\n",
    "        inputs, targets = batch\n",
    "        val_err_batch, val_acc_batch = val_dom_fun(inputs, targets)\n",
    "        _val_err += train_err_batch\n",
    "        _val_acc += val_acc_batch\n",
    "        _val_batches += 1\n",
    "\n",
    "    print (\"\\n  validation domain loss (in-iteration):\\t\\t{:.6f}\".format(_val_err / _val_batches))  \n",
    "    print (\"\\n\\t\\t  validation acc (in-iteration):\\t\\t{:.6f}\".format(_val_acc / _val_batches))  \n",
    "    print (\"\\n\\t\\t  {:.3f}s\".format(time.time() - start_time))\n",
    "     \n",
    "    \n",
    "    ################\n",
    "    #  validation  #\n",
    "    ################\n",
    "    # Again after domain transfer\n",
    "    \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm_notebook(iterate_minibatches(X_val, y_val, batch_size), \n",
    "                                total=int(y_val.shape[0] / batch_size), leave=False, \n",
    "                                       desc = 'classifier_validation_on_base_dataset'):\n",
    "        inputs, targets = batch\n",
    "        val_err_batch, val_acc_batch = val_clf_fun(inputs, targets)\n",
    "        val_err += val_err_batch\n",
    "        val_acc += val_acc_batch\n",
    "        val_batches += 1\n",
    "    print (\"\\n  validation on base domain dataset:\")\n",
    "    print (\"\\n\\t\\t  validation clf loss (in-iteration):\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print (\"\\n\\t\\t  val accuracy (in-iteration):\\t\\t{:.6f}\".format(val_acc / val_batches))\n",
    "    print (\"\\n\\t\\t  {:.3f}s\".format(time.time() - start_time))\n",
    "    val_acc_base_.append(val_acc / val_batches)\n",
    "     \n",
    "    \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm_notebook(iterate_minibatches(X_inv_val, y_inv_val, batch_size), \n",
    "                                    total=int(y_inv_val.shape[0]/batch_size), leave=False, \n",
    "                                            desc = 'classifier_validation_on_inverse_dataset'):\n",
    "        inputs, targets = batch\n",
    "        val_err_batch, val_acc_batch = val_clf_fun(inputs, targets)\n",
    "        val_err += val_err_batch\n",
    "        val_acc += val_acc_batch\n",
    "        val_batches += 1\n",
    "    print (\"\\n  validation on inverse dataset:\")    \n",
    "    print (\"\\n\\t\\t  validation clf loss (in-iteration):\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print (\"\\n\\t\\t  val accuracy (in-iteration):\\t\\t{:.6f}\".format(val_acc / val_batches))\n",
    "    print (\"\\n\\t\\t  {:.3f}s\".format(time.time() - start_time))\n",
    "    val_acc_inv_.append(val_acc / val_batches)\n",
    "    \n",
    "    lambd *= 0.9\n",
    "    \n",
    "#     with open('some_file_for_output1.txt', 'a') as f:\n",
    "#         f.write(\"\\n  Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "#         f.write(\"\\n  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "#         f.write(\"\\n  validation loss (in-iteration):\\t\\t{:.6f}\".format(_train_err / _train_batches))\n",
    "#         f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(val_acc_base_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(val_acc_inv_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Изменить порядок тренировки - сначала домейн, потом классификатор ??? чтобы не ломалось?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.log(10) == 2.3025850929940459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_targ(X_dom_val[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://stackoverflow.com/questions/33879736/can-i-selectively-invert-theano-gradients-during-backpropagation\n",
    "\n",
    "https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/lasagne-users/16H8zBNxoZ0/_nFS637pCAAJ\n",
    "\n",
    "Thinking again, should not Theano be smart enough to do the right thing if you just provide two loss functions?\n",
    "\n",
    "Taking the \"Unsupervised Domain Adaptation by Backpropagation\" paper as an example,\n",
    "your network will be something like:\n",
    "l_in = InputLayer(...)\n",
    "...\n",
    "l_sourcetask = DenseLayer(...)\n",
    "...\n",
    "l_domainclass = DenseLayer(...)\n",
    "\n",
    "I.e., you've got one output layer for the source task classification, and another output layer for\n",
    "the domain classification, and both share the same input layer (and a part of the network).\n",
    "You'd then define two ordinary loss functions:\n",
    "pred_sourcetask, pred_domainclass = lasagne.layers.get_output([l_sourcetask, l_domainclass])\n",
    "loss_sourcetask = lasagne.objectives.categorical_cross_entropy(pred_sourcetask, target_sourcetask)\n",
    "loss_domainclass = lasagne.objectives.categorical_cross_entropy(pred_domainclass, target_domainclass)\n",
    "\n",
    "\n",
    "\n",
    "And use that to update the networks as done in the paper:\n",
    "params1 = lasagne.layers.get_all_params(l_sourcetask)\n",
    "params2 = lasagne.layers.get_all_params(l_domainclass)\n",
    "common = set(params1) & set(params2)\n",
    "\n",
    "updates1 = nesterov_momentum(loss_sourcetask - lambda * loss_domainclass, params1, ...)\n",
    "updates2 = nesterov_momentum(loss_domainclass, list(set(params2) - common), ...)\n",
    "updates3 = nesterov_momentum(-lambda * loss_domainclass, list(common), ...)\n",
    "updates1.update(updates2)\n",
    "updates2.update(updates3)\n",
    "\n",
    "train1 = theano.function([l_in.input_var, target_sourcetask, target_domainclass], [], updates=updates1)\n",
    "train2 = theano.function([l_in.input_var, target_domainclass], [], updates=updates2)\n",
    "\n",
    "Now train1() can be used to train on labeled data from the source task, and train2() to train on \n",
    "unlabeled data from other domains. train1() will update the network to perform well in the source task,\n",
    "make the common representation bad for the domain classification task, but the rest of the domain classifier good \n",
    "for domain classification. train2() will make the common representation bad for the domain classification task \n",
    "and the rest of the domain classifier good for domain classification.\n",
    "Theano should figure out a computation graph that s essentially doing the same as inserting a gradient reversal \n",
    "layer. That s the advantage of having a symbolic graph compiler.\n",
    "\n",
    "Best, Jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
